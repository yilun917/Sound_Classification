{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING PACKAGES\n",
    "\n",
    "import struct\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Add\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Input\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from datetime import datetime \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://medium.com/@mikesmales/sound-classification-using-deep-learning-8bc2aa1990b7\n",
    "# HELPER FUNCTION FOR READING AUDIO FILE\n",
    "class WavFileHelper():\n",
    "    \n",
    "    def read_file_properties(self, filename):\n",
    "\n",
    "        wave_file = open(filename,\"rb\")\n",
    "        \n",
    "        riff = wave_file.read(12)\n",
    "        fmt = wave_file.read(36)\n",
    "        \n",
    "        num_channels_string = fmt[10:12]\n",
    "        num_channels = struct.unpack('<H', num_channels_string)[0]\n",
    "\n",
    "        sample_rate_string = fmt[12:16]\n",
    "        sample_rate = struct.unpack(\"<I\",sample_rate_string)[0]\n",
    "        \n",
    "        bit_depth_string = fmt[22:24]\n",
    "        bit_depth = struct.unpack(\"<H\",bit_depth_string)[0]\n",
    "\n",
    "        return (num_channels, sample_rate, bit_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACTING THE WAVE FORM FROM SOUND TRACK AND CONVERT TO 1-D ARRAY\n",
    "# n is the mfcc value\n",
    "def extract_feature(file_name, n):  \n",
    "    standard_size = 88200  # standard size for audio signal which is about 2 s\n",
    "    try:\n",
    "        audio, sample_rate = librosa.core.load(file_name, mono=True, res_type='kaiser_fast') \n",
    "        fill = standard_size - audio.shape[0]\n",
    "        if(fill>0):\n",
    "            audio = np.concatenate((audio, np.zeros(fill)), axis=0)\n",
    "        elif(fill<0):\n",
    "            audio = audio[:standard_size]\n",
    "            \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n)\n",
    "        # normalize the data\n",
    "        # mfccs = np.mean(mfccs.T,axis=0)\n",
    "        mfccs = librosa.util.normalize(mfccs)  \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None \n",
    "    return mfccs\n",
    "\n",
    "    \n",
    "    \n",
    "# Set the path to the full UrbanSound dataset \n",
    "def audio_extration(n=40):\n",
    "    fulldatasetpath = 'UrbanSound8K/audio/' \n",
    "\n",
    "    metadata = pd.read_csv(fulldatasetpath + 'metadata/UrbanSound8K.csv')\n",
    "\n",
    "    features = []\n",
    "    label_amount = {}\n",
    "\n",
    "    # Iterate through each sound file and extract the features \n",
    "    for index, row in metadata.iterrows():\n",
    "    \n",
    "        file_name = os.path.join(os.path.abspath(fulldatasetpath),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    \n",
    "        class_label = row[\"class\"]\n",
    "        data = extract_feature(file_name, n)\n",
    "        label_amount[class_label] = label_amount.get(class_label, 0) + 1\n",
    "        features.append([data, class_label])\n",
    "\n",
    "\n",
    "    # Convert into a Panda dataframe \n",
    "    featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "\n",
    "    #print('Finished feature extraction from ', len(featuresdf), ' files')\n",
    "    return featuresdf, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT THE CHANNELS, SAMPLE RATE AND BIT DEPTH DATA FROM SOUNDS FILE\n",
    "def info_extration(metadata):\n",
    "    wavfilehelper = WavFileHelper()\n",
    "\n",
    "    audiodata = []\n",
    "    for index, row in metadata.iterrows():\n",
    "    \n",
    "        file_name = os.path.join(os.path.abspath('UrbanSound8K/audio/'),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "        data = wavfilehelper.read_file_properties(file_name)\n",
    "        audiodata.append(data)\n",
    "\n",
    "    # Convert into a Panda dataframe\n",
    "    audiodf = pd.DataFrame(audiodata, columns=['num_channels','sample_rate','bit_depth'])\n",
    "    return audiodf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint(type(featuresdf.iloc[0]['feature']))\\nprint(featuresdf.head())\\nprint()\\nprint(audiodf.head())\\n\\nlst = [0,1,10,23,96,106,114,122,171,196]\\nfor n in lst:\\n    import matplotlib.pyplot as plt\\n    %matplotlib inline  \\n    plt.plot(featuresdf.iloc[n]['feature'][5:])\\n    plt.ylabel('Amplitude')\\n    plt.show()\\n    print(featuresdf.iloc[n]['class_label'])\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(type(featuresdf.iloc[0]['feature']))\n",
    "print(featuresdf.head())\n",
    "print()\n",
    "print(audiodf.head())\n",
    "\n",
    "lst = [0,1,10,23,96,106,114,122,171,196]\n",
    "for n in lst:\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline  \n",
    "    plt.plot(featuresdf.iloc[n]['feature'][5:])\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "    print(featuresdf.iloc[n]['class_label'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEPERATING DATA TO TRAINING AND TESTING WITH 80% 20% RATIO\n",
    "def data_seperation(featuresdf):\n",
    "    # Convert features and corresponding classification labels into numpy arrays\n",
    "    X = np.array(featuresdf.feature.tolist())\n",
    "    y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "    # Encode the labels using Onehot technique\n",
    "    lable_encoder = LabelEncoder()\n",
    "    y_one_hot = to_categorical(lable_encoder.fit_transform(y)) \n",
    "\n",
    "    # split the dataset \n",
    "    train_features, validate_features, train_labels, validate_labels = train_test_split(X, y_one_hot, test_size=0.11, random_state = 42)\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(train_features, train_labels, test_size=0.12, random_state = 42)\n",
    "    #print(\"Training data set size: \",train_features.shape[0])\n",
    "    #print(\"Validate data set size: \",validate_features.shape[0])\n",
    "    #print(\"Test data set size: \",test_features.shape[0])\n",
    "    return (train_features, validate_features, test_features, train_labels, validate_labels, test_labels, y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data for easy access\n",
    "def save_data(n, train_features, validate_features, test_features, train_labels, validate_labels, test_labels, y_one_hot):\n",
    "    pickle_file = 'audio_features_mfcc'+ str(n) +'.pickle'\n",
    "    if not os.path.isfile(pickle_file):\n",
    "        print('Saving data to pickle file...')\n",
    "        try:\n",
    "            with open(pickle_file, 'wb') as pfile:\n",
    "                pickle.dump(\n",
    "                    {\n",
    "                        'train_dataset': train_features,\n",
    "                        'train_labels': train_labels,\n",
    "                        'valid_dataset': validate_features,\n",
    "                        'valid_labels': validate_labels,\n",
    "                        'test_dataset': test_features,\n",
    "                        'test_labels': test_labels,\n",
    "                        'y_one_hot': y_one_hot,\n",
    "                    },\n",
    "                    pfile, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', pickle_file, ':', e)\n",
    "            raise\n",
    "\n",
    "    print('Data cached in pickle file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL BUILDING\n",
    "# n is mfcc number\n",
    "def model_building(n, validate_features, validate_labels, y_one_hot):\n",
    "    num_rows = n  # audio frequency spectrum 173*n\n",
    "    num_columns = 173   \n",
    "    num_channels = 1   # combine 2 channels to one channel\n",
    "    \n",
    "    validate_features = validate_features.reshape(validate_features.shape[0], num_rows, num_columns, num_channels) \n",
    "    num_labels = y_one_hot.shape[1]\n",
    "    \n",
    "    filter_size = 2   \n",
    "    drop_rate = 0.4  # dropping 40% of data to prevent over-fitting\n",
    "\n",
    "    # Construct model \n",
    "    model = Sequential()\n",
    "    # Layer 1 - Conv Layer 1\n",
    "    model.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(drop_rate))\n",
    "\n",
    "    # Layer  - Conv Layer 2\n",
    "    model.add(Conv2D(filters=32, kernel_size=2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(drop_rate))\n",
    "\n",
    "    # Layer 3 - Conv Layer 3\n",
    "    model.add(Conv2D(filters=64, kernel_size=2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(drop_rate))\n",
    "\n",
    "    # Layer 4 - Conv Layer 4\n",
    "    model.add(Conv2D(filters=128, kernel_size=2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    \n",
    "    # Layer 5 - Flatten Layer\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(num_labels, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    \n",
    "    # Pre-training evaluation\n",
    "    \n",
    "    # Display model architecture summary \n",
    "    # model.summary()\n",
    "\n",
    "    # Calculate pre-training accuracy \n",
    "    # score = model.evaluate(validate_features, validate_labels, verbose=1)\n",
    "    # accuracy = 100*score[1]\n",
    "\n",
    "    # print(\"Pre-training accuracy: %.4f%%\" % accuracy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def model_training(n, model, train_features, train_labels, validate_features, \n",
    "                   validate_labels, num_epochs=150, num_batch_size=256):\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath='weights.best.basic_cnn.hdf5', verbose=1, save_best_only=True)\n",
    "    \n",
    "    num_rows = n  # audio frequency spectrum 173*40\n",
    "    num_columns = 173   \n",
    "    num_channels = 1   # combine 2 channels to one channel\n",
    "    train_features = train_features.reshape(train_features.shape[0], num_rows, num_columns, num_channels) \n",
    "    validate_features = validate_features.reshape(validate_features.shape[0], num_rows, num_columns, num_channels)   \n",
    "    \n",
    "    #start = datetime.now()\n",
    "\n",
    "    history = model.fit(train_features, train_labels, batch_size=num_batch_size, epochs=num_epochs, validation_data=(validate_features, validate_labels), shuffle=True, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "    #duration = datetime.now() - start\n",
    "    #print(\"Training completed in time: \", duration)\n",
    "    return history\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MobileNet model\n",
    "def mobileNet_building(n, validate_features, validate_labels, y_one_hot):\n",
    "    num_rows = n  # audio frequency spectrum 173*n\n",
    "    num_columns = 173   \n",
    "    num_channels = 3 \n",
    "    \n",
    "    drop_rate = 0.4  # dropping 40% of data to prevent over-fitting\n",
    "              \n",
    "    input_img = Input(shape=(num_rows, num_columns, num_channels))\n",
    "    mn = MobileNetV2(input_shape=(num_rows, num_columns, num_channels), include_top=False, weights='imagenet',input_tensor=input_img, pooling='max')\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(mn)\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Compile the MobileNet model\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    model.summary()\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def mobileNet_training(n, model, train_features, train_labels, validate_features, \n",
    "                   validate_labels, num_epochs=150, num_batch_size=256):\n",
    "    \n",
    "    #start = datetime.now()\n",
    "\n",
    "    history = model.fit(train_features, train_labels, batch_size=num_batch_size, epochs=num_epochs, validation_data=(validate_features, validate_labels), shuffle=True, verbose=1)\n",
    "\n",
    "    #duration = datetime.now() - start\n",
    "    #print(\"Training completed in time: \", duration)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def load_data(pickle_file_path):\n",
    "    file_dict = pickle.load( open( pickle_file_path, \"rb\" ))\n",
    "    return file_dict['train_dataset'], file_dict['valid_dataset'],\\\n",
    "file_dict['test_dataset'],file_dict['train_labels'],\\\n",
    "file_dict['valid_labels'], file_dict['test_labels'], file_dict['y_one_hot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipe Line\n",
    "# experimenting mfcc number\n",
    "def pip(n):\n",
    "    # data preprocessing\n",
    "    '''\n",
    "    featuresdf, metadata = audio_extration(n)\n",
    "    audiodf = info_extration(metadata)\n",
    "    train_features, validate_features, test_features, \\\n",
    "    train_labels, validate_labels, test_labels, y_one_hot = data_seperation(featuresdf)\n",
    "    save_data(n, train_features, validate_features, test_features, train_labels, validate_labels, test_labels, y_one_hot)\n",
    "    '''\n",
    "    # load pre-processed data\n",
    "    pickle_file_path = 'audio_features_mfcc40.pickle'\n",
    "    train_features, validate_features, test_features, train_labels,\\\n",
    "    validate_labels, test_labels, y_one_hot = load_data(pickle_file_path)\n",
    "    \n",
    "    # Simple model\n",
    "    #model = model_building(n, validate_features, validate_labels, y_one_hot)\n",
    "    #history = model_training(n, model, train_features, train_labels, validate_features, \n",
    "    #                         validate_labels, num_epochs=50, num_batch_size=256)\n",
    "    \n",
    "    # MobileNet\n",
    "    train_features = np.stack((train_features, train_features, train_features), axis=3)\n",
    "    \n",
    "    validate_features = np.stack((validate_features, validate_features, validate_features), axis=3)\n",
    "    print(train_features.shape)\n",
    "    print(validate_features.shape)\n",
    "    model = mobileNet_building(n, validate_features, validate_labels, y_one_hot)\n",
    "    history = mobileNet_training(n, model, train_features, train_labels, validate_features, \n",
    "                             validate_labels, num_epochs=50, num_batch_size=256)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate the effect of between MFCC number\n",
    "mfcc_num = list(range(40,260,20))\n",
    "accuracy = []\n",
    "history_list = []\n",
    "for n in mfcc_num:\n",
    "    history = pip(n)\n",
    "    history_list.append(history)\n",
    "    accuracy.append(history.history['val_acc'][-1])\n",
    "\n",
    "plt.plot(mfcc_num, accuracy)\n",
    "plt.title('Model Accuracy vs. MFCC Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('MFCC Number')\n",
    "plt.savefig('Model Accuracy vs. MFCC Number.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = 'training_history.pickle'\n",
    "if not os.path.isfile(pickle_file):\n",
    "    print('Saving data to pickle file...')\n",
    "    try:\n",
    "        with open(pickle_file, 'wb') as pfile:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                        'history_list': history_list,\n",
    "\n",
    "                },\n",
    "                pfile, pickle.HIGHEST_PROTOCOL)\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', pickle_file, ':', e)\n",
    "        raise\n",
    "\n",
    "print('Training history saved to pickle file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc = []\n",
    "mfcc = list(range(40,140,20))\n",
    "for hist in history_list:\n",
    "    acc.append(hist.history['val_acc'][-1])\n",
    "\n",
    "plt.plot(mfcc, acc)\n",
    "plt.title('Model Accuracy vs. MFCC Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('MFCC Number')\n",
    "plt.savefig('Model Accuracy vs. MFCC Number.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6838, 40, 173, 3)\n",
      "(961, 40, 173, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yilun_mac/anaconda3/lib/python3.7/site-packages/keras_applications/mobilenet_v2.py:294: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9412608/9406464 [==============================] - 1s 0us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_224 (Model) (None, 1280)              2257984   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                12810     \n",
      "=================================================================\n",
      "Total params: 2,270,794\n",
      "Trainable params: 2,236,682\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n",
      "Train on 6838 samples, validate on 961 samples\n",
      "Epoch 1/50\n",
      "6656/6838 [============================>.] - ETA: 10s - loss: 1.5571 - acc: 0.5846"
     ]
    },
    {
     "ename": "AbortedError",
     "evalue": "Operation received an exception:Status: 3, message: could not create a dilated convolution forward descriptor, in file tensorflow/core/kernels/mkl_conv_ops.cc:1111\n\t [[{{node mobilenetv2_1.00_224/Conv1/convolution}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAbortedError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-9f72ebbf0691>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# training the model based on pretrained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-4df78ca6dfd7>\u001b[0m in \u001b[0;36mpip\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmobileNet_building\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     history = mobileNet_training(n, model, train_features, train_labels, validate_features, \n\u001b[0;32m---> 30\u001b[0;31m                              validate_labels, num_epochs=50, num_batch_size=256)\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-08d9193391a8>\u001b[0m in \u001b[0;36mmobileNet_training\u001b[0;34m(n, model, train_features, train_labels, validate_features, validate_labels, num_epochs, num_batch_size)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#start = datetime.now()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#duration = datetime.now() - start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[1;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                                              verbose=0)\n\u001b[0m\u001b[1;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAbortedError\u001b[0m: Operation received an exception:Status: 3, message: could not create a dilated convolution forward descriptor, in file tensorflow/core/kernels/mkl_conv_ops.cc:1111\n\t [[{{node mobilenetv2_1.00_224/Conv1/convolution}}]]"
     ]
    }
   ],
   "source": [
    "# training the model based on pretrained weights\n",
    "history = pip(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "#print(history.history.keys()) \n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validate'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(train_features, train_labels, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(validate_features, validate_labels, verbose=0)\n",
    "print(\"Validation Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "# Save model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"sound_classification_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_weights.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "# load json and create model\n",
    "json_file = open('sound_classification_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_weights.h5\")\n",
    "print(\"Loaded model from disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final testing after the model is tunned\n",
    "test_features = test_features.reshape(test_features.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "score = model.evaluate(test_features, test_labels, verbose=0)\n",
    "\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loadoaded model\n",
    "loaded_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test performance of user input audio file\n",
    "# x need to be numpy array of shape (40, 173) processed through mfcc\n",
    "# make sure the sound file length is <= 2s, or doesn't contians info after 2s\n",
    "file_path = ''\n",
    "x = extract_feature(file_path)\n",
    "print(x.shape)\n",
    "predictions = loaded_model.predict(x)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step:\n",
    "# 1. Run the model\n",
    "# 2. Save the data set (train/validation/test data)\n",
    "# 3. Save the model\n",
    "# 4. Improve model performance\n",
    "# 5. Modify the model - possible: VGG16, LeNet, GoogLeNet\n",
    "# 6. Predict the user input audio clip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
